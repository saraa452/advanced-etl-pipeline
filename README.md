# Advanced ETL Pipeline

[![Python 3.11](https://img.shields.io/badge/python-3.11-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/license-Apache%202.0-green.svg)](LICENSE)

Um pipeline ETL avanÃ§ado e modular para processamento de dados.

## ğŸš€ Funcionalidades

- ExtraÃ§Ã£o de mÃºltiplas fontes (API, Banco de Dados)
- TransformaÃ§Ã£o e enriquecimento de dados
- Carregamento em mÃºltiplos destinos
- Logging e monitoramento
- Testes automatizados
- DockerizaÃ§Ã£o

## ğŸ“¦ InstalaÃ§Ã£o

```bash
git clone https://github.com/seu-usuario/advanced-etl-pipeline.git
cd advanced-etl-pipeline
pip install -r requirements.txt
```

## ğŸ—ï¸ Estrutura do Projeto

```
advanced-etl-pipeline/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extractors/          # MÃ³dulos de extraÃ§Ã£o de dados
â”‚   â”‚   â”œâ”€â”€ api_extractor.py     # Extrator de APIs REST
â”‚   â”‚   â”œâ”€â”€ database_extractor.py # Extrator de bancos de dados
â”‚   â”‚   â””â”€â”€ base_extractor.py    # Classe base abstrata
â”‚   â”œâ”€â”€ transformers/        # MÃ³dulos de transformaÃ§Ã£o
â”‚   â”‚   â””â”€â”€ data_transformer.py  # Transformador de dados
â”‚   â”œâ”€â”€ loaders/             # MÃ³dulos de carregamento
â”‚   â”‚   â”œâ”€â”€ database_loader.py   # Carregador para banco de dados
â”‚   â”‚   â”œâ”€â”€ file_loader.py       # Carregador para arquivos
â”‚   â”‚   â””â”€â”€ base_loader.py       # Classe base abstrata
â”‚   â”œâ”€â”€ utils/               # UtilitÃ¡rios
â”‚   â”‚   â”œâ”€â”€ logger.py            # ConfiguraÃ§Ã£o de logging
â”‚   â”‚   â””â”€â”€ monitoring.py        # Monitoramento de pipeline
â”‚   â””â”€â”€ pipeline.py          # Orquestrador do pipeline ETL
â”œâ”€â”€ tests/                   # Testes automatizados
â”œâ”€â”€ config/                  # Arquivos de configuraÃ§Ã£o
â”œâ”€â”€ Dockerfile               # ConfiguraÃ§Ã£o Docker
â”œâ”€â”€ docker-compose.yml       # ComposiÃ§Ã£o de serviÃ§os
â””â”€â”€ requirements.txt         # DependÃªncias Python
```

## ğŸ“– Uso

### Exemplo BÃ¡sico

```python
from src.pipeline import ETLPipeline
from src.extractors import APIExtractor
from src.transformers import DataTransformer
from src.loaders import FileLoader

# Criar pipeline
pipeline = ETLPipeline("meu_pipeline")

# Configurar extrator
extractor = APIExtractor(
    base_url="https://api.exemplo.com",
    headers={"Authorization": "Bearer token"}
)

# Configurar transformador
transformer = DataTransformer()
transformer.add_transformation(lambda df: df.drop_duplicates())
transformer.add_transformation(lambda df: df.fillna(0))

# Configurar carregador
loader = FileLoader(output_dir="./output")

# Montar e executar pipeline
metrics = (
    pipeline
    .add_extractor(extractor)
    .set_transformer(transformer)
    .add_loader(loader)
    .run(
        extract_kwargs=[{"endpoint": "/dados"}],
        load_kwargs=[{"filename": "dados_processados", "file_format": "csv"}]
    )
)

print(f"Pipeline concluÃ­do: {metrics['status']}")
```

### ExtraÃ§Ã£o de Banco de Dados

```python
from src.extractors import DatabaseExtractor

extractor = DatabaseExtractor(
    connection_string="postgresql://user:password@localhost:5432/database"
)
extractor.connect()
data = extractor.extract(table="usuarios", columns=["id", "nome", "email"])
extractor.disconnect()
```

### TransformaÃ§Ãµes DisponÃ­veis

```python
from src.transformers import DataTransformer

transformer = DataTransformer()

# Limpeza de dados
transformer.clean_data(data, drop_duplicates=True, fill_na=0)

# Renomear colunas
transformer.rename_columns(data, {"old_name": "new_name"})

# Filtrar linhas
transformer.filter_rows(data, "valor > 100")

# Adicionar coluna
transformer.add_column(data, "nova_coluna", lambda df: df["valor"] * 2)

# Converter tipos
transformer.convert_types(data, {"data": "datetime", "valor": "float"})

# Agregar dados
transformer.aggregate(data, group_by=["categoria"], aggregations={"valor": "sum"})
```

### Carregamento para MÃºltiplos Destinos

```python
from src.loaders import DatabaseLoader, FileLoader

# Carregador de banco de dados
db_loader = DatabaseLoader(
    connection_string="postgresql://user:password@localhost:5432/database"
)
db_loader.connect()
db_loader.load(data, table="tabela_destino", if_exists="append")
db_loader.disconnect()

# Carregador de arquivos (CSV, JSON, Parquet)
file_loader = FileLoader(output_dir="./output")
file_loader.connect()
file_loader.load(data, filename="dados", file_format="csv")
file_loader.disconnect()
```

## ğŸ§ª Testes

Executar todos os testes:

```bash
pytest tests/ -v
```

Executar com cobertura:

```bash
pytest tests/ -v --cov=src --cov-report=term-missing
```

## ğŸ³ Docker

### Construir imagem

```bash
docker build -t etl-pipeline .
```

### Executar com Docker Compose

```bash
# Iniciar todos os serviÃ§os
docker-compose up -d

# Executar testes no container
docker-compose run test

# Parar serviÃ§os
docker-compose down
```

## ğŸ“Š Monitoramento

O pipeline inclui monitoramento integrado:

```python
from src.utils.monitoring import PipelineMonitor

monitor = PipelineMonitor("meu_pipeline")
monitor.start_pipeline()

monitor.start_stage("extract")
# ... operaÃ§Ãµes de extraÃ§Ã£o
monitor.end_stage(records_processed=1000)

monitor.start_stage("transform")
# ... operaÃ§Ãµes de transformaÃ§Ã£o
monitor.end_stage(records_processed=950)

metrics = monitor.end_pipeline(success=True)
print(monitor.get_summary())
```

## ğŸ“ ConfiguraÃ§Ã£o

Edite o arquivo `config/config.yaml` para personalizar:

- ConfiguraÃ§Ãµes de banco de dados
- ParÃ¢metros de API
- DiretÃ³rios de saÃ­da
- NÃ­veis de logging

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a Apache License 2.0 - veja o arquivo [LICENSE](LICENSE) para detalhes.
